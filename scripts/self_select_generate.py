from managers.data_manager import DataManager
from managers.llm_manager import LlmManager
from managers.session_manager import SessionManager
from managers.prompt_manager import PromptManager
from .self_select_summary import summarize_all

# necessary modules
import pandas as pd
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
import datetime as dt
import json
import tqdm
import time
import openpyxl
import base64
from io import BytesIO


def gen_trend_report_customized(title: str, start_date: str, end_date: str, user_name, user_email, raw_data, cols, additional, uploaded_data = pd.DataFrame()) -> json:
    if not uploaded_data.empty:
        assert 'é‡é»æ‘˜è¦' in uploaded_data.columns, "ä½¿ç”¨è€…ä¸Šå‚³çš„è³‡æ–™ä¸­æ²’æœ‰'é‡é»æ‘˜è¦'æ¬„ä½"
        assert 'é—œéµæ•¸æ“š' in uploaded_data.columns, "ä½¿ç”¨è€…ä¸Šå‚³çš„è³‡æ–™ä¸­æ²’æœ‰'é—œéµæ•¸æ“š'æ¬„ä½"
    """
    Before:
        raw_data ç‚ºIIIè³‡æ–™åº«çš„ä¸­çš„æ–°èè³‡æ–™ã€‚æ­¤å‡½æ•¸æœ€é–‹å§‹æœƒå…ˆåµæ¸¬è©²æ‰¹è³‡æ–™çš„æ‘˜è¦æ˜¯å¦å­˜åœ¨è³‡æ–™åº«ï¼Œè‹¥å¦ï¼Œå†ç”¨ summarize_all() å‡½æ•¸ä¾†è£½ä½œã€‚
    After:
        è‹¥è¦å°‡æ­¤å·¥å…·èˆ‡ä½¿ç”¨è€…ä¸Šå‚³çš„è³‡æ–™å“¡é€²è¡Œæ•´åˆï¼Œå‰‡å¿…é ˆè¦è€ƒæ…®ä½¿ç”¨è€…è³‡æ–™çš„ä¸Šå‚³ï¼Œå› ç‚ºæ‘˜è¦è³‡æ–™æ‡‰è©²è¦åŒ…å«æ‰€æœ‰çš„è³‡æ–™æºï¼Œä½†æœ¬ä¾†çš„åµæ¸¬æ–¹æ³•æœƒå¿½ç•¥ä½¿ç”¨è€…ä¸Šå‚³çš„è³‡æ–™ï¼ˆex: è£½ä½œå…©æ¬¡ 3/1 - 3/8 çš„ç§‘æŠ€è¶¨å‹¢ï¼Œä½†ä½¿ç”¨è€…ä¸Šå‚³çš„è³‡æ–™æœ‰æ‰€ä¸åŒã€‚è€Œè©²æ‰¹æ‘˜è¦è³‡æ–™å·²ç¶“åœ¨ç¬¬ä¸€æ¬¡è¢«è£½ä½œä¸¦ä¸”å›å‚³è‡³è³‡æ–™åº«ï¼Œå› æ­¤ç¬¬äºŒæ¬¡è£½ä½œæ™‚æœƒç›´æ¥è·³éæ‘˜è¦éšæ®µï¼Œæœƒç”¢ç”Ÿå•é¡Œï¼‰ã€‚

        å¯ä»¥è§€å¯Ÿåˆ°åœ¨ summarize_all() å‡½æ•¸ä¸­ï¼Œå¼•æ•¸ raw_data åªæœ‰ã€Œé‡é»æ‘˜è¦ã€å’Œã€Œé—œéµæ•¸æ“šã€å…©æ¬„æœƒè¢«ä½¿ç”¨ï¼Œå› æ­¤å¯ä»¥åœ¨è¼¸å…¥çµ¦ summarize_all() å‡½æ•¸å‰å…ˆå°‡ raw_data èˆ‡ä½¿ç”¨è€…ä¸Šå‚³çš„è³‡æ–™éƒ½åªç•™ä¸‹é€™å…©æ¬„ï¼Œå†é€²è¡Œå‚ç›´åˆä½µï¼Œæœ€å¾Œä¸€æ¬¡è¼¸å…¥ summarize_all()ã€‚ä½†ç•¶ä½¿ç”¨è€…ä¸Šå‚³è³‡æ–™ç‚º Noneï¼Œå‰‡ä½¿ç”¨åŸæœ¬æ–¹æ³•ã€‚
    """
    # *** Summary Data Generation ***
    # -- data è®Šæ•¸æŒ‡çš„æ˜¯å·²ç¶“æ‘˜è¦å®Œæˆçš„æ–°èè³‡æ–™ï¼Œæ˜¯ summarize_all() çš„å›å‚³çµæœ
    if uploaded_data.empty:
        try:
            data = DataManager.b64_to_dataframe(DataManager.get_files(f"Summary_{title}_{start_date}-{end_date}.xlsx", 'xlsx'))  
        except:
            data = summarize_all(raw_data, user_name, user_email, title, start_date, end_date)

    else:
        if raw_data.empty:
            merged_raw = uploaded_data
        else:
            merged_raw = pd.concat([raw_data[['é‡é»æ‘˜è¦', 'é—œéµæ•¸æ“š']], uploaded_data[['é‡é»æ‘˜è¦', 'é—œéµæ•¸æ“š']]])
        data = summarize_all(merged_raw, user_name, user_email, title, start_date, end_date)

    # -------------------------------------------------------------------------------------
    st_bar = st.progress(0, text = f"Generating {title} trend report...")
    # *** Generate three versions of trend reports ***
    st_bar.progress(0, text = f"Generating {title} trend report (ver 1)")
    try:
        three_vers = st.session_state['self_select_three_vers']
    except:
        in_message = ""
        for index, row in data.iterrows():
            if row[f"é‡é»æ–°èæ‘˜è¦"] != "" :
                in_message += row["batch"] + "\n" + '\n'.join(row[f"é‡é»æ–°èæ‘˜è¦"]) + "\n"
    
        # progress bar: there are 6 steps so the denominator is 6
        st_bar.progress(0, text = f"Generating {title} trend report (three versions)...")
            
        three_vers = {}
        for ver in range(1, 4):
            chain = LlmManager.create_prompt_chain(PromptManager.SELF_SELECT.step2_prompt(title, ver, additional), st.session_state['model'])
            three_vers.update(LlmManager.llm_api_call(chain, in_message.rstrip()))
            
            st_bar.progress(1/5 * (ver/3), f"({round(1/5 * (ver/3) * 100)}%) Generating {title} trend report (ver {ver + 1})")
        st.session_state['self_select_three_vers'] = three_vers

    # -------------------------------------------------------------------------------------

    # *** Aggregate three versions into one ***
    st_bar.progress(1/5, text = f"(20%) Aggregating three version {title} trend report")
    try:
        trends_basic = st.session_state['self_select_trends_basic']
    except:

        in_message = ""
        for ver, value in three_vers.items():
            in_message = in_message + '\n' + ver + '\n\n' + json.dumps(value) + '\n\n'

        chain = LlmManager.create_prompt_chain(PromptManager.SELF_SELECT.step3_prompt(additional), 
                                               st.session_state['model'])
        trends_basic = LlmManager.llm_api_call(chain, in_message.rstrip())

        st.session_state['self_select_trends_basic'] = trends_basic

    # -------------------------------------------------------------------------------------
    # *** Classify representative events to corresponding trend ***
    # events in the topic to be classified
    st_bar.progress(2/5, text = f"(40%) Classifying events to each trend...")
    try:
        trends_with_events = st.session_state['self_select_trends_with_events']
    except:
        events_message = ""
        for index, row in data.iterrows():
            if row[f"é‡é»æ–°èæ‘˜è¦"] != "" :
                events_message += row["batch"] + "\n" + '\n'.join(row[f"é‡é»æ–°èæ‘˜è¦"]) + "\n"

        # classify events to each trend
        trends_with_events = []
        count = 1
        for name, trend in trends_basic.items():
            trend_message = json.dumps(trend)
            in_message = f'''
            ç¸½å…±æœ‰é€™äº›äº‹ä»¶ã€‚è«‹åˆ†é¡ï¼š\n\n{events_message}\n\n----\n\n*****ä¸»è¦è¶¨å‹¢{count}ï¼ˆ{name}ï¼‰*****ï¼š\n\n{trend_message}
            '''
            chain = LlmManager.create_prompt_chain(PromptManager.SELF_SELECT.step4_prompt(additional),
                                                   st.session_state['model'])
            trends_with_events.append(LlmManager.llm_api_call(chain, in_message.rstrip()))
            st_bar.progress(2/5 + 1/5 * (count/len(trends_basic)), text = f"({round((2/5 + 1/5 * (count/len(trends_basic))) * 100)}%) Classifying events to each trend..." )
            count += 1

        st.session_state['self_select_trends_with_events'] = trends_with_events

    # -------------------------------------------------------------------------------------
    
    # *** Inference ! é‡é ­æˆ² ***
    st_bar.progress(3/5, text = f"(60%) Inferring {title} trend report...")
    try:
        trend_inference = st.session_state['self_select_trend_inference']
    except:
        trend_inference = {}
        count = 1
        for trend in trends_with_events:
            in_message = json.dumps(trend)
            chain = LlmManager.create_prompt_chain(PromptManager.SELF_SELECT.step5_prompt(cols, additional),
                                                   st.session_state['model'])
            trend_inference.update(LlmManager.llm_api_call(chain, in_message.rstrip()))
            st_bar.progress(3/5 + 1/5 * (count/len(trends_with_events)), text = f"({round((3/5 + 1/5 * (count/len(trends_with_events))) * 100)}%) Inferring {title} trend report..." )
            count += 1
        st.session_state['self_select_trend_inference'] = trend_inference
    # -------------------------------------------------------------------------------------

    # *** Final summarization *** 
    st_bar.progress(4/5, text = f"Concluding {title} trend report...")
    try:
        final_summary = st.session_state['self_select_final_summary']
        
    except:
        in_message = ""
        for name, trend in trend_inference.items():
            message = f'''{name}: \n\n1. è¶¨å‹¢æ´å¯Ÿï¼š\n{trend['<a>è¶¨å‹¢æ´å¯Ÿ']}\n\n2. é‡è¦é—œä¿‚äººï¼š\n{trend['<d>é‡è¦é—œä¿‚äºº']}\n\n3. 
            '''#ç¼ºå£ï¼š{trend["<e>ç¼ºå£"]}\n\n
            in_message += message

        chain = LlmManager.create_prompt_chain(PromptManager.SELF_SELECT.step6_prompt(additional),
                                               st.session_state['model'])
        final_summary = LlmManager.llm_api_call(chain, in_message.rstrip())
        st.write('''    ğŸ¤ Final summarization completed! ''')
        st_bar.progress(1, text = f"(100%) Complete!")
        st_bar.empty()
        st.session_state['self_select_final_summary'] = final_summary
    # -------------------------------------------------------------------------------------
        

    result_json = DataManager.merge_dict(final_summary, trend_inference)
    encoded_json = base64.b64encode(json.dumps(result_json).encode('utf-8')).decode('utf-8')

    DataManager.post_files(file_name = f"{title}_trend_report_{start_date}-{end_date}.json", 
                           file_content = encoded_json, 
                            expiration = str(dt.datetime.today() + dt.timedelta(90)), 
                            user_name = user_name, 
                            user_email = user_email)

    return result_json


def get_key_data_per_report(title, trend_report_json, pdf_text, pdf_file_name):
    """
    å‰é¢æ­¥é©Ÿå·²ç”Ÿæˆçš„è¶¨å‹¢å ±å‘Š + ä½¿ç”¨è€…ä¸Šå‚³çš„ç ”ç©¶èª¿æŸ¥å ±å‘Š(pdf) -> é‡å°æ¯å€‹è¶¨å‹¢ï¼Œå¾å–®ä¸€ pdf ä¸­æå–é—œéµæ•¸æ“š / æ¡ˆä¾‹
    """
    chain = LlmManager.create_prompt_chain(PromptManager.SELF_SELECT.get_key_data_from_pdf(title, trend_report_json),
                                               st.session_state['model'])
    pdf_output = {pdf_file_name: LlmManager.llm_api_call(chain, in_message = pdf_text.rstrip())}
    st.session_state['pdfs_output'].update(pdf_output)